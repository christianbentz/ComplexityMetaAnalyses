---
title: "Simulation Study: Negative Correlations and Equality of Means"
author: ''
date: "February 10, 2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Session Info
Give the session info (reduced).
```{r, echo = F}
# R version
sessionInfo()$R.version$version.string
# platform
sessionInfo()$R.version$platform 
```

# Load Libraries
If the libraries are not installed yet, you need to install them using, for example, the command: install.packages("ggplot2").
```{r, message = FALSE}
library(MASS)
library(ggplot2)
library(plyr)
library(GGally)
library(rstatix)
```

Give the package versions.
```{r, echo = F}
# version of packages loaded
installed.packages()[names(sessionInfo()$otherPkgs), "Version"]
```

# Introduction
The purpose of this file is two-fold: a) It illustrates the decisions we take to statistically analyse the respective data, i.e. checking for normality of the data, choosing a statistical test for assessing equality in the median values of two distributions, and getting effect sizes for this test. b) It discusses under which conditions negative correlations (i.e. trade-offs) entail equi-complexity in the sense of a non-significant difference in the medians of two distributions. Both is achieved by using simulated data, i.e. pseudo-complexity measurements for two languages A and B, for which a negative correlation is pre-defined.  

# Theoretical Background

## Trade-offs as Negative Correlations
Trade-offs are here conceptualized as negative correlations. We here choose the Pearson correlation coefficient r as an example. For samples from two random variables X and Y it is defined as

\begin{equation}
r_{xy}= \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \overline{x})^2} \sqrt{\sum_{i=1}^{n}(y_i - \overline{y})^2}}.
\end{equation}

Where $n$ is the number of data points in the paired samples, $x_i$ and $y_i$ are individual data points, and $\overline{x}$ and $\overline{y}$ are the sample means. We have a negative correlation $r_{xy} < 0$ iff the numerator is negative, i.e. 

\begin{equation}
\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y}) < 0.  
\end{equation}

Note that the denominator cannot be negative.

## Equi-Complexity as Equality of Means
Furthermore, we conceptualize equi-complexity as equality of means. Assume that the $x_i$s and $y_i$s represent complexity measurements for two languages A and B. We would then consider A and B equi-complex if 

\begin{equation}
\overline{x} = \overline{y},  
\end{equation}

where 

\begin{equation}
\overline{x} = \frac{1}{n}\sum_{i=1}^{n} x_i,  
\end{equation}

and 

\begin{equation}
\overline{y} = \frac{1}{n}\sum_{i=1}^{n} y_i.  
\end{equation}

## Linear Transformation
We here proof that a negative Pearson correlation between two samples $x_i$ and $y_i$ does not strictly entail equality of means, i.e. $\overline{x} = \overline{y}$. 


# Generate Correlated Data
We here generate correlated pseudo-complexity measurements for two languages A and B. We then apply linear and non-linear transformations to illustrate how this impacts the results of correlation and mean value analyses. 
```{r}
# set the seed for random number generation in order to 
# get the same result when the code is re-run
set.seed(1)
# set parameters
n = 20 # number of datapoints
r = -0.7 # predefined correlation
a = 2 # constant for linear transformation
# generate the data
data <- mvrnorm(n = n, mu = c(3, 3), Sigma = matrix(c(1, r, r, 1), nrow = 2), 
                empirical = TRUE)
langA <- data[, 1]
langB <- data[, 2]
# apply linear transformation to language B measures
langB.lt <- langB*a
# apply non-linear transformation to language B measures
langB.nt <- sqrt(langB)
```

Standardize the data by centering and scaling it.
```{r}
langA.stand <- scale(langA)
langB.stand <- scale(langB)
langB.lt.stand <- scale(langB.lt)
langB.nt.stand <- scale(langB.nt)
```

Put into short format (for scatterplots) and long format data frame (density distributions).
```{r}
# short format
simulation.df.short <- data.frame(langA, langB, langB.lt, langB.nt, langA.stand, langB.stand, 
                                  langB.lt.stand, langB.nt.stand)
# long format
# non-standardized
value <- c(langA, langB, langB.lt, langB.nt)
measurement <- rep(c(1:n), times = 4)
language <- c(rep("Language A", times = n), 
              rep("Language B", times = n),
              rep("Language B (linear trans.)", times = n), 
              rep("Language B (non-linear trans.)", times = n)
              )
simulation.df.long <- data.frame(language, measurement, value)
head(simulation.df.long)
# standardized
value <- c(langA.stand, langB.stand, langB.lt.stand, langB.nt.stand)
measurement <- rep(c(1:n), times = 4)
language <- c(rep("Language A (standardized)", times = n), 
              rep("Language B (standardized)", times = n),
              rep("Language B (linear trans., standardized)", times = n),
              rep("Language B (non-linear trans., standardized)", times = n)
              )
simulation.df.long.stand <- data.frame(language, measurement, value)
head(simulation.df.long.stand)
```

# Scatterplot with Correlations
```{r, fig.width = 10, fig.height = 10, messages = FALSE, warning = FALSE}
simulation.scatterplot <- ggpairs(simulation.df.short, 
                        lower = list(continuous = wrap("smooth_loess", alpha = 0.3, 
                                                       lwd = 0.5, size = 2))) +
                        #upper = list(continuous = wrap('cor', method = "spearman"))) +
                        theme_bw() +
                        theme(axis.text.x = element_text(angle = 90, hjust = 1))
print(simulation.scatterplot)
```

# Density Distributions
Plot density distributions of complexity pseudo-measurements by language. Individual values for each complexity pseudo-measurement are plotted as black dots. The central value (0) is indicated by a vertical dotted line for visual reference. The median and mean values of complexity pseudo-measurements per language might also be indicated.

## Non-Standardized Vectors
Get mean, median, and standard deviation values.
```{r}
# get mean values for each language
mu <- ddply(simulation.df.long, "language", summarise, grp.mean = mean(value, na.rm = T))
# get median values for each language
med <- ddply(simulation.df.long, "language", summarise, grp.median = median(value, na.rm = T))
# get standard deviation values for each language
sdev <- ddply(simulation.df.long, "language", summarise, grp.sd = sd(value, na.rm = T)) 
```

Plot density distributions with indication of median (mean) values.
```{r, fig.width = 6, fig.height = 4}
density.plot <- ggplot(simulation.df.long, aes(x = value)) +
 geom_density(alpha = .2, fill = "grey", color = "darkgrey") +
 geom_jitter(data = simulation.df.long, aes(x = value, y = 0), 
             size = 1, height = 0.03, width = 0) + # add some jitter to prevent overplotting
  geom_vline(aes(xintercept = 3), color = "darkgrey") +
  geom_vline(data = med, aes(xintercept = grp.median), linetype = "dashed") +
  facet_wrap(~ language) +
  #xlim(-3, 3) +
  labs(x = "Complexity Value", y = "Density") +
  theme_bw() +
  theme(legend.position = "none") 
print(density.plot)
```

## Standardized Vectors
Get mean, median, and standard deviation values.
```{r}
# get mean values for each language
mu <- ddply(simulation.df.long.stand, "language", summarise, grp.mean = mean(value, na.rm = T))
# get median values for each language
med <- ddply(simulation.df.long.stand, "language", summarise, grp.median = median(value, na.rm = T))
# get standard deviation values for each language
sdev <- ddply(simulation.df.long.stand, "language", summarise, grp.sd = sd(value, na.rm = T)) 
```

Plot density distributions with indication of median (mean) values.
```{r, fig.width = 6, fig.height = 4}
density.plot.stand <- ggplot(simulation.df.long.stand, aes(x = value)) +
 geom_density(alpha = .2, fill = "grey", color = "darkgrey") +
 geom_jitter(data = simulation.df.long.stand, aes(x = value, y = 0), 
             size = 1, height = 0.03, width = 0) + # add some jitter to prevent overplotting
  geom_vline(aes(xintercept = 0), color = "darkgrey") +
  geom_vline(data = med, aes(xintercept = grp.median), linetype = "dashed") +
  facet_wrap(~ language) +
  #xlim(-3, 3) +
  labs(x = "Complexity Value", y = "Density") +
  theme_bw() +
  theme(legend.position = "none") 
print(density.plot.stand)
```

Save figure to file.
```{r, fig.width = 7, fig.height = 2.5}
#ggsave("Figures/Simulation/scatterplot.pdf", density.plot, dpi = 300, scale = 1, 
#       device = cairo_pdf)
```

## Normality
The assumption that the tested data stems from a normally distributed population is often necessary for the mathematical proofs underlying standard statistical techniques. We might apply normality tests to check for this assumption (e.g. Baayen 2008, p. 73), but some statisticians advise against such pre-tests, since they are often too sensitive (MacDonald 2014, p. 133-136, Rasch et al. (2020), p. 67). In fact, Rasch et al. (2020, p. xi) argue based on earlier simulation studies that almost all standard statistical tests are fairly robust against deviations from normality. However, it is still advisable to check for gross deviations from normality in the data. One common way of doing this is quantile-quantile plots. The points should here roughly follow a straight line (Crawley 2007, p. 281).

```{r, fig.width = 3.5, fig.height = 3}
ggplot(simulation.df.long, aes(sample = value)) + 
  stat_qq()
```

## Choose statistical tests
Select a statistical test:
Standard t-tests can be used to assess significant differences in the means of the pseudo-complexity distributions, if we assume that the underlying population distributions are normal. Wilcoxon tests are a non-parametric alternative, i.e. they do not make assumptions about the underlying population distribution, e.g. normality (Crawley 2007, p. 283; Baayen 2008, p. 77). 

Run pairwise Wilcoxon tests.
```{r, message = F}
# we add some random noise here to the value vector with
# the function jitter(), since we otherwise get warnings due to ties in the data
p.values <- pairwise.wilcox.test(jitter(simulation.df.long$value), simulation.df.long$language, 
                     paired = F, p.adjust.method = "holm") 
p.values$p.value
```

# Effect Size
Statistical significance is only one part of the story. For instance, a difference in complexity values might be statistically significant, but so small that it is negligible for any theorizing. In fact, it is sometimes argued that effect sizes - rather than p-values - should be the aim of statistical inquiry (Cahusac 2020, p. 12-15). An overview of effect size measures per statistical test is given in Patil (2020). In conjunction with the Wilcoxon signed rank test we here use the statistic r (i.e. function wilcox_effsize() of the "rstatix" package). 

```{r}
# non-standardized
effect.sizes <- wilcox_effsize(simulation.df.long, value ~ language, paired = F)
# standardized
effect.sizes.stand <- wilcox_effsize(simulation.df.long.stand, value ~ language, paired = F)
#print(effect.sizes)
```

# Effect Size Heatmap
Plot a heatmap with effect sizes to get a better overview.
```{r, fig.width = 4.2, fig.height = 2.8}
# non-standardized
effect.sizes.plot <- ggplot(as.data.frame(effect.sizes), aes(group1, group2)) +
  geom_tile(aes(fill = effsize), color = "white") +
  scale_fill_gradient2(low = "light blue", mid = "light grey", high = "red", 
                       midpoint = 0.5, limit = c(0,1)) +
  geom_text(aes(label = round(effsize, 2))) +
  labs(x = "", y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
effect.sizes.plot
```

For the standardized vectors.
```{r, fig.width = 5, fig.height = 3.6}
effect.sizes.plot.stand <- ggplot(as.data.frame(effect.sizes.stand), aes(group1, group2)) +
  geom_tile(aes(fill = effsize), color = "white") +
  scale_fill_gradient2(low = "light blue", mid = "light grey", high = "red", 
                       midpoint = 0.5, limit = c(0,1)) +
  geom_text(aes(label = round(effsize, 2))) +
  labs(x = "", y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
effect.sizes.plot.stand
```

# References
Baayen, R. H. (2008). Analyzing linguistic data: A practical introduction using statistics in R. Cambridge University Press.

Cahusac, P. M. B. (2021). Evidence-based statistics. John Wiley & Sons.

Crawley, M. J. (2007). The R book. John Wiley & Sons Ltd.

McDonald, J.H. (2014). Handbook of Biological Statistics (3rd ed.). Sparky House Publishing, Baltimore, Maryland. online at http://www.biostathandbook.com

Patil, I. (2020). Test and effect size details. online at https://cran.r-project.org/web/packages/statsExpressions/vignettes/stats_details.html.

Rasch, D., Verdooren, R., and J. Pilz (2020). Applied statistics. Theory and problem solutions with R. John Wiley & Sons Ltd.