---
title: "Correlations Between Complexity Measures"
author: "Chris Bentz"
date: "January 22, 2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Libraries
If the libraries are not installed yet, you need to install them using, for example, the command: install.packages("ggplot2").
```{r, message = FALSE}
library(readr)
library(ggplot2)
library(gridExtra)
library(GGally)
library(ggrepel)
library(psych)
library(corrplot)
```

# Load the Data 
The participants' results are loaded as csv files directly from the github repository into separate data frames. We only use the name of the first author (lower case) to name the data frame.
```{r, message = FALSE}
#Track A (Parallel Bible Corpus, PBC)
gutierrez.results <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/PBCtrack/Gutierrez/Gutierrez.csv")
# remove the parentheses in column names
colnames(gutierrez.results) <- sub("\\(", "", colnames(gutierrez.results)) 
colnames(gutierrez.results) <- sub("\\)", "", colnames(gutierrez.results))
# replace "+" by "."
colnames(gutierrez.results) <- gsub("\\+", ".", colnames(gutierrez.results)) 
oh.results <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/PBCtrack/Oh/oh.csv")

#TRACK B (Universal Dependencies, UD)
brunato.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Brunato_venturi/Brunato-Venturi.csv")
coltekin.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Coltekin_rama/coltekin.csv")
semenuks.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Semenuks/Semenuks.csv")
sinnemaki.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Sinnemaki/Sinnemaki.csv")
sozinova.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Sozinova_etal/sozinova_etal.csv")
```

Sanity check, look at the number of rows and columns of the data frames.
```{r}
#Track A (should be 49 rows)
track.a.rows <- c(nrow(gutierrez.results), nrow(oh.results))
print(track.a.rows) # this corresponds to the number of languages
track.a.cols <- c(ncol(gutierrez.results), ncol(oh.results))
print(track.a.cols) # this is the number of measures per team

#Track B (should be 63 rows)
track.b.rows <- c(nrow(brunato.results), nrow(coltekin.results), nrow(semenuks.results), nrow(sinnemaki.results), nrow(sozinova.results))
print(track.b.rows) # this corresponds to the number of languages
track.b.cols <- c(ncol(brunato.results), ncol(coltekin.results), ncol(semenuks.results), ncol(sinnemaki.results), ncol(sozinova.results))
print(track.b.cols) # this is the number of measures per team
```

# Preprocessing
Put data into a single data frame.
```{r}
track.a <- cbind(gutierrez.results, oh.results[, 3:ncol(oh.results)])
track.b <- cbind(brunato.results, coltekin.results[, 3:ncol(coltekin.results)], 
                 semenuks.results[, 3:ncol(semenuks.results)], 
                 sinnemaki.results[, 3:ncol(sinnemaki.results)], 
                 sozinova.results[, 3:ncol(sozinova.results)])
```

Remove certain measures.
```{r}
# remove measures in TRACK A which are redundant (since the only difference is whether the parallel corpora 
# are fully paralellized or not)
track.a <- track.a[ , -which(names(track.a) %in% c("GM_H1gram", "GM_H3gram", "GM_TTR",
                                                   "GM_TTR.H1", "GM_TTR.H3", "GM_TTR.H1.H3"))]
# remove measures in TRACK B with many NAs
track.b <- track.b[ , -which(names(track.b) %in% c("SI_double_dl", "SI_head_dl", "SI_zero_dl"))]
```

Invert the values (by substracting them from 1) for the measure "CR_inflection_accurracy" in Track B. Note that higher values in the original measure mean *lower* rather than higher complexity.
```{r}
track.b$CR_inflection_accuracy <- 1-track.b$CR_inflection_accuracy
```

Check the first 6 rows of the data.
```{r}
#head(track.a)
#head(track.b)
```

Remove the first two columns of data frames (useful for plotting).
```{r}
track.a.short <- track.a[, 3:ncol(track.a)]
track.b.short <- track.b[, 3:ncol(track.b)]
```

# Scatterplots by Track

## TRACK A
For visual reference, we here firstly give scatterplots between selected measures of the respective track (trying to exclude the ones which are somewhat redundant). The Spearman correlation coefficient is reported instead of the Pearson correlation coefficient. This is because we are only interested whether there is a correlation between the rankings of complexities, regardless of whether this is a linear relationship. We therefore also use the local regression smoothers in the plots (loess) rather than linear models (lm). Note: warning messages are disabled here as there are datasets with NAs, and for each plot this throws a warning message using the ggpairs() plotting function. NAs are delt with by removing the entire row containing an NA value. 

```{r, fig.width = 15, fig.height = 15, warning = FALSE}
track.a.scatterplot <- ggpairs(track.a.short, 
                        lower = list(continuous = wrap("smooth_loess", alpha = 0.3, 
                                                       lwd = 0.5, size = 2))) +
                        #upper = list(continuous = wrap('cor', method = "spearman"))) +
                        theme_bw() +
                        theme(axis.text.x = element_text(angle = 90, hjust = 1))
print(track.a.scatterplot)
```

Safe plot to file.
```{r, warning = FALSE}
ggsave("~/Github/ComplexityMetaAnalyses/Figures/Corrs/TrackA/track_a_scatterplot.pdf", track.a.scatterplot, 
       dpi = 300, scale = 1, width = 15, height = 15, device = cairo_pdf)
```

## TRACK B
Same for the Track B data. Not all measures are included here (there would be 24). To include them all, the "columns" argument in the code below might be removed.
```{r, fig.width = 15, fig.height = 15, warning = FALSE}
track.b.scatterplot <- ggpairs(track.b.short, progress = TRUE,
                        lower = list(continuous = wrap("smooth_loess", alpha = 0.3, 
                                                       lwd = 0.5, size = 2)),
                        upper = list(continuous = wrap('cor', method = "spearman")),
                        columns = c("BV_n_tokens", "BV_char_per_tok", "BV_avg_links_len",
                              "BV_avg_max_depth", "CR_inflection_accuracy", "CR_ttr", 
                              "S_idMean", "S_idSD", "SI_dm", "SI_hm", "SI_dep_dl", 
                              "SBS_INF", "SBS_DER")) +
                        theme_bw() +
                        theme(axis.text.x = element_text(angle = 90, hjust = 1))
print(track.b.scatterplot)
```

Safe plot to file.
```{r, warning = FALSE}
ggsave("~/Github/ComplexityMetaAnalyses/Figures/Corrs/TrackB/track_b_scatterplot.pdf", track.b.scatterplot, 
       dpi = 300, scale = 1, width = 15, height = 15, device = cairo_pdf)
```

# Significant Correlations after Correction for Multiple testing
Not all of the correlations displayed above are going to be significant after correcting for multiple testing. We therefore use the corr.test() function here, since it enables us to choose a correction method, i.e. Holm-Bonferroni. The Bonferroni method would be more conservative, however, it is pointed out in (MacDonald 2014, p. 254-260) that it is appropriate only when tests are independent of one another. Since we here run pairwise tests by complexity measures, our tests are not independent (the same measure is tested against others multiple times). We therefore apply the Holm-Bonferroni method (see also the descriptions in the vignette invoked by the command "?p.adjust()"). Note that NAs are here deleted in pairs of columns, rather than across a whole row. 

## TRACK A
```{r, fig.width = 5, fig.height = 5}
cor.results <- corr.test(track.a.short, method = "spearman", 
                         use = "pairwise.complete.obs", adjust = "holm")
# define color range
col <- colorRampPalette(c("darkred", "white", "steelblue"))(20)
# give correlation plot
corrplot(cor.results$r, type = "upper", order = "hclust", col = col,
         tl.col = "black", tl.cex = 0.5,
         addCoef.col = "black", number.cex = 0.5,
         p.mat = cor.results$p,
         pch.cex = 2,
         #pch.col = "gray",
         sig.level = 0.05, insig = "pch")
```

## TRACK B
Same as above for Track A.
```{r, fig.width = 9, fig.height = 9}
cor.results <- corr.test(track.b.short, method = "spearman", 
                         use = "pairwise.complete.obs", adjust = "holm")
# define color range
col <- colorRampPalette(c("darkred", "white", "steelblue"))(20)
# give correlation plot
corrplot(cor.results$r, type = "upper", order = "hclust", col = col,
         tl.col = "black", tl.cex = 0.5,
         addCoef.col = "black", number.cex = 0.5,
         p.mat = cor.results$p,
         pch.cex = 2,
         #pch.col = "gray",
         sig.level = 0.05, insig = "pch")
```

# Detailed Scatterplots 
We here plot the highest *positive* and *negative* correlations (in terms of Spearman coefficients) which are still significant after the Bonferroni correction *and* which are found between measures proposed by *different participants* (there are many measures by the same participants that highly correlate). These are hand-picked from the correlograms above.

# TRACK A (Highest Positive Correlation)
```{r, fig.width = 5, fig.height = 5, warning = FALSE}
#track.a <- track.a[track.a$id != "mya", ] # remove the outlier Burmese (mya)

track.a.plot1.detailed <- ggplot(track.a, aes(x = GM_TTR_fullyparallelised, y = O_WID)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(method = loess, alpha = 0.3) +
  geom_label_repel(data = track.a[track.a$language == "Fijian" | track.a$language == "Sango" | track.a$language == "Vietnamese" | track.a$language == "Vietnamese" | track.a$language == "English" | track.a$language == "Georgian" |  track.a$language == "Russian" | track.a$language == "Swahili" | track.a$language == "Basque" | track.a$language == "Finnish" | track.a$language == "Turkish" | track.a$language == "Korean" | track.a$language == "Kalaallisut" | track.a$language == "Burmese", ],
                   min.segment.length = 0,
                   #nudge_x = 0.1,
                   aes(label = language), 
                       size = 3) +
  theme(legend.position = "none")
track.a.plot1.detailed
```

Some comments: This plot shows that the Type-Token Ratio (TTR) and the Word Information Density (WID) are highly correlated across the languages of the Parallel Bible Corpus sample. Burmese (mya) is an outlier here with very high TTR and WID. This is an artifact of the writing system, since it does not delimit orthographic words by white spaces, but rather phrases. For Kalaallisut, on the other hand, the result makes sense (if we accept the latinized writing proposed for this language). Some of the low TTR languages include Sango (sag), Fijian (fij), Thai (tha), and Yoruba (yor).   

# TRACK B (Highest Positive Correlation)
```{r, fig.width = 5, fig.height = 5, warning = FALSE}
#track.b <- track.b[track.b$id != "uig", ] # remove the outlier Uyghur (uig)

track.b.plot1.detailed <- ggplot(track.b, aes(x = CR_msp, y = SBS_INF)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(method = loess, alpha = 0.3) +
  geom_label_repel(data = track.b[track.b$language == "Chinese" | track.b$language == "Vietnamese" | track.b$language == "English" | track.b$language == "Russian" | track.b$language == "Old Church Slavonic" | track.b$language == "Basque" | track.b$language == "Finnish" | track.b$language == "Turkish" | track.b$language == "Latin" | track.b$language == "Uyghur" | track.b$language == "Ancient Greek", ],
                   min.segment.length = 0,
                   #nudge_x = 0.1,
                   aes(label = language), 
                   size = 3) +
  theme(legend.position = "none")
track.b.plot1.detailed
```

Some comments: This plot shows the correlation between the so-called Mean Size of Morphological Paradigms (MSP), which is defined by CR as "simply the number of word-form types divided by the number of lemma types", and the difference in unigram entropy of word tokens in the original texts and the lemmatized texts (INF) as defined by SBS. It is certainly not unexpected, but reassuring, to see these measure highly correlated. The outlier to the high end Uyghur (uig) is likely *not* an artifact, as this language indeed has many productive morphological paradigms. Other languages to the high end of morphological complexity include Ancient Greek (grc), Classical Latin (lat), Turkish (tur), and Old Church Slavonic (chu). Languages to the low end are Vietnamese (vie), Indonesian (ind), Mandarin Chinese (cmn), and Afrikaans (afr). Note that the very low morphological complexity scores of Korean (kor) are an artifact of the way the Korean data is presented in the UD. Namely, the "lemmas" given for Korean are actually merely morphologically segmented forms rather than inflectionally neutralized forms as for the other languages. Thus, it makes sense that the MSP is exactly 1 and the INF is 0. 

# TRACK A (Highest Negative Correlation)
```{r, fig.width = 5, fig.height = 5, warning = FALSE}
track.a.negative.detailed <- ggplot(track.a, aes(x = GM_TTR_fullyparallelised, y = O_SID)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(method = loess, alpha = 0.3) +
  geom_label_repel(data = track.a,
                   min.segment.length = 0,
                   #nudge_x = 0.1,
                   aes(label = language), 
                       size = 3) +
  theme(legend.position = "none") +
  xlim(0.05, 0.4)
track.a.negative.detailed
```

# Track B (Highest Negative Correlation)
```{r, fig.width = 5, fig.height = 5, warning = FALSE}
track.b <- track.b[track.b$language != "Korean", ] # remove the outlier Korean

track.b.scatterplot.negative.detailed <- ggplot(track.b, aes(x = CR_mfe, y = SI_dep_dl)) + 
  geom_point(alpha = 0.3) +
  geom_smooth(method = loess, alpha = 0.3) +
  geom_label_repel(data = track.b[track.b$language == "Chinese" | track.b$language == "Vietnamese" | track.b$language == "German" | track.b$language == "English" | track.b$language == "Hungarian" | track.b$language == "Greek" | track.b$language == "Russian" | track.b$language == "Old Church Slavonic" | track.b$language == "Basque" | track.b$language == "Finnish" | track.b$language == "Turkish" | track.b$language == "Korean" | track.b$language == "Latin" | track.b$language == "Uyghur" | track.b$language == "Ancient Greek", ],
                    min.segment.length = 0,
                   #nudge_x = 0.1,
                   aes(label = language), 
                   size = 3) +
  theme(legend.position = "none")
track.b.scatterplot.negative.detailed
```

```{r, fig.width = 10, fig.height = 10, warning = FALSE}
ggsave("~/Github/ComplexityMetaAnalyses/Figures/Corrs/TrackB/track_b_scatterplot_negative_detailed.pdf", track.b.scatterplot.negative.detailed, dpi = 300, scale = 1, width = 5, height = 5, device = cairo_pdf)
```

# Conclusions
Some more general observations based on these analyses include:

- Many of the measures proposed by the same participants highly correlate. This is the case, for instance, for the measures proposed by GM in Track A, but also measures of BV in Track B. In the case of GM, this is because many of the measures are virtually the same, but with minor shades of modification. In the case of BV, while at first sight the measures seem to conceptually differ, they essentially boil down to the same underlying causes. For example, the number of tokens in a sentence highly predicts the average maximal depth of a tree over the sentence. So, arguably most of these positive intra-participant correlations are driven by redundancy in the proposed measures. 

- There are several strong positive correlations between simple measures relating to the number of types and tokens (GM_TTR_fullyparallelised, BV_n_tokens, etc.), and measures of information density (O_WID, S_idSD). Interestingly, this is the case for both tracks, since Oh used the Bible texts, and Semenuks used the UD. Information density is generally assumed to be a measure which has psycholinguistic relevance in terms of language processing. However, the fact that it is highly predictable by some of the simplest word frequency measures (TTR) potentially goes to show that the underlying principles driving complexity are fairly similar.    
- We have mainly discussed positive correlations here, meaning that certain measures are essentially (better or worse) replacements of other measures. In fact, the majority of correlations still significant after the Bonferroni correction are positive (in Track A all of them are positive, in Track B 37 out of 49 are positive). Some of the negative correlations we do find are between CR's "inflection accuracy" and different measures of inflectional complexity (CR_mfe, SBS_INF, CR_msp). This makes perfect sense given that inflection accuracy is a measure that reflects the difficulty of NLP tools to automatically deal with inflectional morphology. The more complex the morphology, the lower the accuracy of the automated tool. A negative correlation that seems both robust and potentially interesting is that the dependency lengths in noun phrases with marked possessives (SI_dep_dl) apparently are in a clear trade-off with different measures of inflectional complexity. However, the fact that there are few such instances of robust negative correlations between measures of different domains suggests that there are relatively few clear complexity trade-offs. This is sth. to further think about.    

# References
McDonald, J.H. (2014). Handbook of Biological Statistics (3rd ed.). Sparky House Publishing, Baltimore, Maryland. online at http://www.biostathandbook.com
