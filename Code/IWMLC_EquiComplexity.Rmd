---
title: 'IWMLC: Meta-Analyses of Complexity Results'
author: "Chris Bentz"
date: "January 06, 2021"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## LOAD PACKAGES
Load packages. If they are not installed yet on your local machine, use install.packages() to install them.
```{r, message = FALSE}
library(readr)
library(MASS)
library(tidyr)
library(ggplot2)
library(scales)
library(plyr)
library(rcompanion)
```

## LOAD DATA
Load results of all participants in the UD track directly from github repo by using the "readr" library.
```{r, message = FALSE}
brunato.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Brunato_venturi/Brunato-Venturi.csv")
coltekin.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Coltekin_rama/coltekin.csv")
semenuks.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Semenuks/Semenuks.csv")
sinnemaki.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Sinnemaki/Sinnemaki.csv")
sozinova.results  <- read_csv("https://raw.githubusercontent.com/IWMLC/language-complexity-metrics/master/UDtrack/Sozinova_etal/sozinova_etal.csv")
```

## DATA PRE-PROCESSING
Create a single data frame merging results together using the Reduce() and merge() functions of base R.
```{r}
results <- Reduce(merge, list(brunato.results, coltekin.results, semenuks.results, sinnemaki.results, sozinova.results)) # by default the merge() function uses columns with the same names to merge on, i.e. "id" and "language" in our case.
```

Give some simple statistics for this data frame of results.
```{r}
nrow(results) # i.e. number of different UD corpora
length(unique(results$language)) # i.e. number of different languages
unique(results$language) # output the language names
ncol(results)-2 # i.e. number of complexity measures
```

Invert the values (by substracting them from 1) for the measure "CR_inflection_accurracy". Note that higher values in the original measure mean *lower* rather than higher complexity.
```{r}
results$CR_inflection_accuracy <- 1-results$CR_inflection_accuracy
```

Scale all numerical columns to make them more comparable.
```{r}
results.scaled <- cbind(results[1:2], scale(results[3:29])) # add meta-information columns again
```

Transform data frame from wide format to long format (this is necessary for later plotting and analyses by columns rather than rows).
```{r}
results.long <- gather(results.scaled, key = measure, value = value, BV_n_tokens:SBS_DER)
```

Select particular complexity measures (if needed).
```{r}
selection <- c("BV_n_tokens", "BV_verbal_head_per_sent", "BV_verbal_root_perc","BV_avg_token_per_clause",
               "BV_avg_links_len", "BV_avg_max_depth", "BV_avg_verb_edges", "BV_avg_subordinate_chain_len",
               "BV_avg_subordinate_pre", "BV_avg_subordinate_post", "S_idMean", "S_idSD")
results.long <- results.long[results.long$measure %in% selection, ]
```

## DENSITY DISTRIBUTIONS
Plot density distributions of complexity measurements by language. Individual measurements for each complexity measure (and different corpora if available) are plotted as black dots. The central value (0.5) is indicated by a vertical dotted line for visual reference. The median value of complexity measurements per language is indicated as a dashed line.

Get mean, median, and standard deviation values.
```{r}
mu <- ddply(results.long, "language", summarise, grp.mean = mean(value, na.rm = T)) # get mean values for each language 
med <- ddply(results.long, "language", summarise, grp.median = median(value, na.rm = T)) # get median values for each language
sdev <- ddply(results.long, "language", summarise, grp.sd = sd(value, na.rm = T)) # get standard deviation values for each language
```

Plot density distributions with indication of median (mean) values.
```{r, fig.width = 12, fig.height = 10}
density.plot <- ggplot(results.long, aes(x = value)) + 
 #geom_histogram(aes(y = ..density..), colour = "black", fill = "light grey",
                #binwidth = 0.1) +
 geom_density(alpha = .2, fill = "grey", color = "darkgrey") +
 geom_jitter(data = results.long, aes(x = value, y = 0), 
             size = 1, height = 0.1, width = 0) +
 facet_wrap(~ language) +
 # geom_vline(data = mu, aes(xintercept=grp.mean),
 #           linetype = "dotted") +
 geom_vline(data = med, aes(xintercept = grp.median),
             linetype = "dashed", color = "red") +
 geom_vline(aes(xintercept = 0), linetype = "dotted") +
 labs(x = "Complexity Value", y = "Density") +
 xlim(-3, 3) +
 theme_bw()
print(density.plot)
```

Safe figure to file.
```{r, fig.width = 12, fig.height = 10}
#ggsave("Figures/iwmlc_densities.pdf", density.plot, dpi = 300, scale = 1, 
       #device = cairo_pdf)
```

## STATISTICAL TESTS
Run statistical tests to assess whether the aggregate complexity distributions for certain pairs of languages differ signficantly from one another. Note that for this sample of 44 languages there are 44*(44-1)/2 = 946 possible pairwise comparisons. We do not attempt to run them all. We here rather manually choose languages for illustration purposes. 

# Overview of median values
Give an overview of languages with highest and lowest median complexity values.
```{r}
stats.df <- cbind(mu, med[, 2], sdev[, 2])
colnames(stats.df) <- c("language", "mu", "med", "sdev")
stats.df.sorted <- stats.df[order(-stats.df$med),]
print(stats.df.sorted)
```

# Select languages
Select vectors of complexity measurements for particular languages of the results.long data frame.
```{r}
grc.vec <- results.long[results.long$id == "grc_perseus", ]$value
kor.vec <- results.long[results.long$id == "kor_gsd", ]$value
```

# Run Wilcoxon tests
We here use a so-called Wilcoxon signed rank test for two dependent samples. This statistical test assesses whether there is a significant "location shift" in the two distributions of values. This is often associated with differences in the medians (though this is strictly speaking not correct). It is a non-parametric test, i.e. it does not depend on the assumption that the distributions follow a normal distribution. 

```{r}
wilcox.test(grc.vec, kor.vec, paired = T) 
```

# Effect Size
Statistical significance is only one part of the story. For instance, a difference in complexity values might be statistically significant, but so small that it is negligible for any theorizing. In fact, it is sometimes argued that effect sizes - rather than p-values - should be the aim of statistical inquiry (Cahusac 2020, p. 12-15). An overview of effect size measures per statistical test is given in Patil (2020). In conjunction with the Wilcoxon signed rank test we here use the statistic r (i.e. function wilcoxonPairedR() of the "rcompanion" package). 
```{r}
# subset the data frame to contain selected languages only. This is necessary to meet the input structure of the wilcoxonPairedR() function 
value <- c(grc.vec, kor.vec)
id <- c(rep("grc_perseus", ncol(results)-2), rep("kor_gsd", ncol(results)-2))
grc.kor.df <- data.frame(id, value)

# run the effect size estimations
wilcoxonPairedR(x = grc.kor.df$value, g = grc.kor.df$id)
```
# Interpretation: 





